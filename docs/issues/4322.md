# #4322: Optimize compressed size better

- **URL:** https://github.com/WebAssembly/binaryen/issues/4322
- **Author:** kripken
- **Created:** 2021-11-11
- **Updated:** 2021-11-15

## Description

@RReverser noticed that running wasm-opt subsequently on already-optimized output might harm compressibility. A concrete example is this Unreal Engine 4 game:

http://gokuraku.m2.valueserver.jp/teien/HTML5/UE4Game-HTML5-Shipping.wasm

We can be sure that was already optimized because when wasm-opt runs on it it only changes it by a few % (on unoptimized emscripten output, or unoptimized LLVM output more generally, the benefit is more like 30% - huge amounts of opportunity remain in unoptimized builds! - and as UE4 uses emscripten, if it optimizes it would run wasm-opt). Running wasm-opt on it leads to

* 2% improvement in binary size.
* 2% regression in compressed brotli size.

Investigating this, everything wasm-opt does helps both binary size and compressed size, *except* for inlining. The module has quite a lot of functions with a single caller, and `-Os` will inline those - as it can only reduce binary size, and it often makes things faster too. But here at least compressibility is harmed.

It is hard to be sure about the cause here, but looking into it, my impression is that inlining moves code around, taking a function from its original place into a "random" other place, and that "randomness" hurts us. The original place may be "non-random" in that, for example, if the codebase has a physics component then those functions tend to appear together in the output (just because compiling and linking tend to leave object files close together), and those might be similar in shape (lots of float operations, for example).

These regressions appear to only happen when running wasm-opt on already-optimized code, as running it as a normal part of the toolchain should always improve both binary size and compressed size, based on all the data I've ever seen. For example, measuring now on the emscripten benchmark suite, wasm-opt helps binary size by 12% and compressed size by 9%. That is, optimized LLVM outputs still leave some 10% or so opportunity for binaryen in both binary and compressed size. Still, while that is the normal developer workflow and binaryen already helps there, there may be other workflows for running wasm-opt on someone else's binaries, say in a CDN as @RReverser pointed out to me. And, more importantly, likely the inlining pass has always been regressing compressibility, it's just that all the other passes helped so much it wasn't noticed - that is, running wasm-opt on already-optimized output happens to be a case where the main thing wasm-opt finds to do is to inline, and so the effects are more obvious.

It's possible wasm-opt is already doing a good job on that UE4 file, as 2% worse compressibility is traded off for 2% better binary size, which might mean 2% better VM compilation times, less memory in the VM, and also potentially faster execution. But it would be good to improve compressibility too. And the tradeoff isn't always obvious, here is another example:

https://oso-web-demo.s3.us-west-2.amazonaws.com/de568a042820fcd97a6f.module.wasm

There wasm-opt reduces binary size by 1%, which strongly suggests it's already been optimized by wasm-opt (as it seems to be Rust, so it is LLVM output, and see notes earlier on that). But here we regress compressed size by 7.7%. But if we disable inlining then the result is a 0.3% improvement to both binary size and compressed size, so once more inlining is the relevant pass. (Note that the binary size difference suggests that thanks to inlining we were able to optimize away more than 0.5% of the wasm, which suggests there might be a speed win here, and so the tradeoff is not obvious.)

Possible ideas here:

* This brilliant idea from @rfk , https://www.rfk.id.au/blog/entry/cromulate-improve-compressibility/ - basically, find which functions compress well together and sort according to that. I actually started to look into something like that for wasm-opt, years ago, but never had the time to finish it, in the [`similarity-ordering` branch](https://github.com/WebAssembly/binaryen/compare/similarity-ordering?expand=1#diff-ca4ba84bf41fd0c6ca13e4ea46934933084e268fd013c87e08de177d43ef2f16R18-R31). That might be worth picking up. The data on pypy.js show that while this can take a while to do, it can lead to large compressibility wins.

* Another option might be for `-Os / -Oz` to measure compressed size after opts, and if we regress, to not change the binary. Or perhaps we could do so on a pass-by-pass basis, specifically for inlining. But then we'd need to decide what the right tradeoff is (how much compressed size are we willing to lose for a win on all the other metrics).

## Comments (2)

### RReverser (2021-11-12)

For more data, here I've pulled a list of unique Wasm files from the HTTP Archive that are [*] over 100KB uncompressed and where Brotli size regresses by >1% after wasm-opt: https://gist.github.com/RReverser/67f3f90e732368c0fbb875ce695c6490

[*] - clarification: all Wasm files were first stripped of debug info, and then compared with Brotli at level 9 before/after wasm-opt

---

### kripken (2021-11-15)

Another idea from @sunfishcode :

* We could look into improving how well brotli works on wasm, like helping it find a good dictionary (maybe giving it a known good one to start with, for example). Modern compressors do hard work to find a dictionary, reorder blocks, and so forth, and maybe teaching them a little about wasm could help.

---

